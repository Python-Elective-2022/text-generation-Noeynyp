{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9057db-1a4a-4931-bbd7-c01a5eee5546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-29 00:35:22--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.145.102, 142.250.145.101, 142.250.145.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.145.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hoqsndqskeum6k9djaletjccdsfhqiuj/1680050100000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=375910af-47e1-4dc2-8a70-f3f1be72ef1d [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-29 00:35:26--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hoqsndqskeum6k9djaletjccdsfhqiuj/1680050100000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=375910af-47e1-4dc2-8a70-f3f1be72ef1d\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 108.177.127.132, 2a00:1450:4013:c07::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|108.177.127.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   131MB/s    in 0.5s    \n",
            "\n",
            "2023-03-29 00:35:27 (131 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5f7e8c-2853-4a9d-c87b-f854fb112f22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0bcb2b-a897-449c-c68c-f659e9d9202c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1dc0c5-e329-4b30-b097-ec52a4106de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 18s 67ms/step - loss: 6.0197 - accuracy: 0.0318\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 20ms/step - loss: 5.4464 - accuracy: 0.0333\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 5.3760 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.3286 - accuracy: 0.0353\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.2616 - accuracy: 0.0414\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.1913 - accuracy: 0.0429\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 5.1231 - accuracy: 0.0414\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.0601 - accuracy: 0.0525\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 4.9882 - accuracy: 0.0545\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 4.9077 - accuracy: 0.0671\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 4.8225 - accuracy: 0.0721\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.7352 - accuracy: 0.0747\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 4.6546 - accuracy: 0.0853\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.5684 - accuracy: 0.0908\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.4833 - accuracy: 0.0959\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.3971 - accuracy: 0.1060\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.3041 - accuracy: 0.1216\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.2153 - accuracy: 0.1266\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 4.1352 - accuracy: 0.1443\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 4.0555 - accuracy: 0.1625\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 3.9718 - accuracy: 0.1771\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.9012 - accuracy: 0.1917\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.8270 - accuracy: 0.2069\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.7517 - accuracy: 0.2286\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.6720 - accuracy: 0.2538\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5954 - accuracy: 0.2659\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.5286 - accuracy: 0.2856\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 3.4720 - accuracy: 0.2957\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.4008 - accuracy: 0.3027\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 3.3449 - accuracy: 0.3118\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.2746 - accuracy: 0.3295\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.2094 - accuracy: 0.3396\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 3.1371 - accuracy: 0.3592\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.0771 - accuracy: 0.3875\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0137 - accuracy: 0.3956\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.9608 - accuracy: 0.4021\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.9025 - accuracy: 0.4258\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.8364 - accuracy: 0.4354\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7767 - accuracy: 0.4475\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.7269 - accuracy: 0.4531\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.6758 - accuracy: 0.4606\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6043 - accuracy: 0.4803\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5656 - accuracy: 0.4919\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.5064 - accuracy: 0.5040\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.4585 - accuracy: 0.5101\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.4090 - accuracy: 0.5177\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 2.3581 - accuracy: 0.5232\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.3114 - accuracy: 0.5288\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.2732 - accuracy: 0.5484\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.2252 - accuracy: 0.5515\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.1922 - accuracy: 0.5570\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.1643 - accuracy: 0.5646\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 2.1108 - accuracy: 0.5772\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 2.0682 - accuracy: 0.5787\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.0315 - accuracy: 0.5812\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9937 - accuracy: 0.5979\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9528 - accuracy: 0.6039\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9150 - accuracy: 0.6110\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.8664 - accuracy: 0.6181\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.8296 - accuracy: 0.6261\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8019 - accuracy: 0.6387\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.7655 - accuracy: 0.6418\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7390 - accuracy: 0.6493\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.7096 - accuracy: 0.6549\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6807 - accuracy: 0.6579\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6522 - accuracy: 0.6720\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6194 - accuracy: 0.6811\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5857 - accuracy: 0.6842\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.5696 - accuracy: 0.6842\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 1.5480 - accuracy: 0.6857\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.5463 - accuracy: 0.6826\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.4930 - accuracy: 0.6958\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.4893 - accuracy: 0.7069\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.4581 - accuracy: 0.7018\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.4261 - accuracy: 0.7175\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.4052 - accuracy: 0.7109\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4085 - accuracy: 0.7038\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3919 - accuracy: 0.7089\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 1.3557 - accuracy: 0.7210\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3139 - accuracy: 0.7341\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2930 - accuracy: 0.7376\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2683 - accuracy: 0.7392\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2440 - accuracy: 0.7457\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2189 - accuracy: 0.7518\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 1.2227 - accuracy: 0.7497\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2356 - accuracy: 0.7487\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1967 - accuracy: 0.7563\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1777 - accuracy: 0.7659\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 1.1410 - accuracy: 0.7735\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1192 - accuracy: 0.7694\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.1105 - accuracy: 0.7735\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0918 - accuracy: 0.7871\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0753 - accuracy: 0.7830\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0747 - accuracy: 0.7846\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0456 - accuracy: 0.7891\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 1.0183 - accuracy: 0.7952\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.9982 - accuracy: 0.8022\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.9891 - accuracy: 0.8032\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.9720 - accuracy: 0.8052\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9624 - accuracy: 0.8093\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9424 - accuracy: 0.8108\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9273 - accuracy: 0.8158\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9084 - accuracy: 0.8158\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.8939 - accuracy: 0.8214\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.8826 - accuracy: 0.8229\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8729 - accuracy: 0.8199\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8684 - accuracy: 0.8249\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8509 - accuracy: 0.8290\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8541 - accuracy: 0.8229\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8332 - accuracy: 0.8290\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8440 - accuracy: 0.8229\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8471 - accuracy: 0.8254\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8953 - accuracy: 0.8163\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8700 - accuracy: 0.8143\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8304 - accuracy: 0.8244\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8282 - accuracy: 0.8264\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8198 - accuracy: 0.8239\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9885 - accuracy: 0.7725\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8823 - accuracy: 0.8032\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.8086 - accuracy: 0.8224\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.7770 - accuracy: 0.8274\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.7524 - accuracy: 0.8370\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7652 - accuracy: 0.8269\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7359 - accuracy: 0.8355\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7181 - accuracy: 0.8446\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.7080 - accuracy: 0.8471\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.6952 - accuracy: 0.8456\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7718 - accuracy: 0.8330\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7254 - accuracy: 0.8370\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7014 - accuracy: 0.8421\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8339 - accuracy: 0.8169\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7481 - accuracy: 0.8350\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7196 - accuracy: 0.8456\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6827 - accuracy: 0.8476\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.6570 - accuracy: 0.8527\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.6784 - accuracy: 0.8431\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.6475 - accuracy: 0.8522\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.6324 - accuracy: 0.8547\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6164 - accuracy: 0.8582\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.6023 - accuracy: 0.8633\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5942 - accuracy: 0.8633\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5843 - accuracy: 0.8628\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.5773 - accuracy: 0.8628\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.5727 - accuracy: 0.8643\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5736 - accuracy: 0.8663\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5716 - accuracy: 0.8628\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5593 - accuracy: 0.8663\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5476 - accuracy: 0.8718\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.5396 - accuracy: 0.8673\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5352 - accuracy: 0.8698\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5380 - accuracy: 0.8729\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5427 - accuracy: 0.8668\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5428 - accuracy: 0.8683\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5282 - accuracy: 0.8683\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5225 - accuracy: 0.8708\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5165 - accuracy: 0.8734\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5216 - accuracy: 0.8688\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5424 - accuracy: 0.8724\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5180 - accuracy: 0.8739\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5047 - accuracy: 0.8708\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4989 - accuracy: 0.8729\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4909 - accuracy: 0.8754\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4808 - accuracy: 0.8784\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4742 - accuracy: 0.8799\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4713 - accuracy: 0.8824\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4733 - accuracy: 0.8814\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4614 - accuracy: 0.8835\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4578 - accuracy: 0.8814\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4550 - accuracy: 0.8835\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4487 - accuracy: 0.8840\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4573 - accuracy: 0.8804\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4708 - accuracy: 0.8789\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4528 - accuracy: 0.8835\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4860 - accuracy: 0.8764\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4709 - accuracy: 0.8789\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4842 - accuracy: 0.8718\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4520 - accuracy: 0.8799\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4423 - accuracy: 0.8819\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4304 - accuracy: 0.8850\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4252 - accuracy: 0.8865\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4543 - accuracy: 0.8824\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4271 - accuracy: 0.8824\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4157 - accuracy: 0.8885\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4124 - accuracy: 0.8890\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4045 - accuracy: 0.8895\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3993 - accuracy: 0.8885\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4035 - accuracy: 0.8895\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.3962 - accuracy: 0.8910\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3921 - accuracy: 0.8915\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3861 - accuracy: 0.8935\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3824 - accuracy: 0.8915\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.3797 - accuracy: 0.8905\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.3768 - accuracy: 0.8966\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3730 - accuracy: 0.8925\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3728 - accuracy: 0.8935\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3807 - accuracy: 0.8890\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3717 - accuracy: 0.8951\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4199 - accuracy: 0.8809\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4286 - accuracy: 0.8804\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4273 - accuracy: 0.8729\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "64fa3d71-f4a7-4e98-f9a1-237eacd8f477"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df057f68-ae51-4e90-918a-a654735d62d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 664ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "im feeling chills me the sun is still in the pain will end figure lightly lightly love love found ground found eyes song chiquitita eyes lightly love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love love\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}